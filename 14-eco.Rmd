# Ecological use case {#eco}

## Prerequisites {-}

This chapter assumes you have a strong grasp of spatial data analysis and processing, covered in chapters 2-5.
In it you will make use of R's interfaces to dedicated GIS software, and spatial cross validation, topics covered in chapters \@ref(gis) and \@ref(spatial-cv) respectively.

The chapter uses the following packages:

```{r, message=FALSE} 
library(dplyr)
library(mlr)
library(parallelMap)
library(raster)
library(RQGIS)
library(sf)
library(vegan)
```

## Introduction

Fog oases are one of the most fascinating vegetation formations I have ever encountered. 
These formations, locally termed *lomas*, develop on mountains along the coastal deserts of Peru and Chile.
The deserts' extreme conditions and remoteness provide the habitat for a unique ecosystem, including species endemic to the fog oases.
Despite the arid conditions and low levels of precipitation of around 30-50 mm per year on average, plants can survive, by 'combing out' fog.
This fog, which develops below the temperature inversion caused by the cold Humboldt current in austral winter, provides the name for this habitat.
Every few years, the El Niño phenomenon brings torrential rainfall to this sun-baked environment [@dillon_lomas_2003].
This causes the desert to bloom, and provides tree seedlings a chance to develop roots long enough to survive the following arid conditions.

Unfortunately fog oases are heavily endangered.
This is mostly due to human activity (agriculture and climate change).
To effectively protect the last remnants of this unique vegetation ecosystem, evidence is needed on the composition and spatial distribution of the native flora [@muenchow_predictive_2013; @muenchow_soil_2013].
*Lomas* mountains also have economic value as a tourist destination, and can contribute to the wellbeing of local people via recreation.
For example, most Peruvians live in the coastal desert, and *lomas* mountains are frequently the closest "green" destination.

In this chapter we will demonstrate ecological applications of some of the techniques learned in the previous chapters.
This case study will involve analyzing the composition and the spatial distribution of the vascular plants on the southern slope of Mt. Mongón, a *lomas* mountain near Casma on the central northern coast of Peru (Fig. \@ref(fig:study-area-mongon)).

```{r study-area-mongon, echo=FALSE, fig.cap="The Mt. Mongón study area (taken from @muenchow_rqgis:_2017; Landsat image: path 9, row 67, acquisition date 09/22/2000; @usgs_geological_2016).", out.width="60%"}
knitr::include_graphics("https://user-images.githubusercontent.com/1825120/38989956-6eae7c9a-43d0-11e8-8f25-3dd3594f7e74.png")
```

During a field study to Mt. Mongón we recorded all vascular plants living in 100 randomly sampled 4x4 m^2^ plots in the austral winter of 2011 [@muenchow_predictive_2013].
The sampling coincided with a strong La Niña event that year (see ENSO monitoring of the [NOASS Climate Prediction Center](http://origin.cpc.ncep.noaa.gov/products/analysis_monitoring/ensostuff/ONI_v5.php)).
This led to even higher levels of aridity than usual in the coastal desert.
On the other hand, it also increased fog activity on the southern slopes of Peruvian *lomas* mountains.

<!--
The first hypothesis is that four plant belts will be found along the altitudinal gradient: a low-elevation *Tillandsia* belt, a herbaceous belt, a bromeliad belt, and an uppermost succulent belt [@muenchow_soil_2013].
-->

Ordinations are dimension-reducing techniques which allow the extraction of the main gradients from a (noisy) dataset, in our case the floristic gradient developing along the southern mountain slope.
In this chapter we will try to model the first ordination axis, i.e., the floristic gradient, as a function of environmental predictors such as altitude, slope, catchment area and NDVI.
The corresponding model will allow us to make spatial predictions of the floristic composition anywhere in the study area.
To retrieve bias-reduced performance estimates, we will of course account for the likely presence of spatial autocorrelation with the help of spatial cross-validation (see Chapter \@ref(spatial-cv)).

## Data and data preparation
All the data needed for the subsequent analyses is available via the **RQGIS** package.

```{r}
data("random_points", "comm", "study_area", "dem", "ndvi")
```

`study_area` is an sf polygon representing the outlines of the study area.
`random_points` is an sf-object, and contains the 100 randomly chosen sites.
`comm` is a community matrix where the rows represent the visited sites and the columns the observed species.
^[In statistics this is also called a contingency or cross-table, and in data science we refer to this as the wide data format.]
The values represent species cover per site, and were recorded as the area covered by a species in proportion to the site area in percentage points.
The rownames of `comm` correspond in fact to the `id` column of `random_points`.
Though `comm` only consists of 86 rows, we have in fact visited 100 sites in the field, however, in 16 of them no species were found.
`dem` is the digital elevation model for the study area, and `ndvi` is the Normalized Difference Vegetation Index (NDVI) computed from the red and near-infred channels of a Landsat scene (see section \@ref(local-operations) and `?ndvi`).
Visualizing the data helps to get more familiar with the data:

```{r, eval=FALSE, echo=FALSE}
# create hillshade
hs = hillShade(terrain(dem), terrain(dem, "aspect"))
# plot the data
par(mar = rep(1, 4))
plot(hs, col = gray(0:100 / 100), legend = FALSE, axes = FALSE, box = FALSE)
plot(dem, axes = FALSE, add = TRUE, alpha = 0.5, legend = FALSE)
axis(1)
axis(2)
plot(st_geometry(random_points), add = TRUE)
plot(st_geometry(study_area), add = TRUE)
# white margins between axes and plot are too wide
```

```{r, echo=FALSE, fig.cap="Location of the sampling sites in the study area, study mask and used DEM as a hillshade."}
library("latticeExtra")
library("grid")
hs = hillShade(terrain(dem), terrain(dem, "aspect"))
spplot(dem, col.regions = terrain.colors(50), alpha.regions = 0.5,
       scales = list(draw = TRUE,
                     tck = c(1, 0)),
       colorkey = list(space = "right", title = "m asl",
                               width = 0.5, height = 0.5,
                       axis.line = list(col = "black")),
       sp.layout = list(
         list("sp.points", as(random_points, "Spatial"), pch = 16,
              col = "black", cex = 0.8, first = FALSE),
         list("sp.polygons", as(study_area, "Spatial"), 
              col = "black", first = FALSE)
       )
       ) + 
  latticeExtra::as.layer(spplot(hs, col.regions = gray(0:100 / 100)), 
                         under = TRUE)
grid.text("m asl", x = unit(0.8, "npc"), y = unit(0.75, "npc"), 
          gp = gpar(cex = 0.8))

```


The next step is to compute variables which we will predominantly need for the modeling and predictive mapping (see section \@ref(predictive-mapping)) but also for aligning the NMDS axes with the main gradient, altitude and humidity, respectively, in the study area (see section \@ref(nmds)).

The terrain attributes can be computed from a digital elevation model using R-GIS bridges (see Chapter \@ref(gis)).
Specifically, we will compute catchment slope and catchment area.
Curvatures might also represent valuable predictors, in the exercise section you can find out how they would change the modeling result.

To compute catchment area and catchment slope, we will make use of the `saga:sagawetnessindex` function.
^[Admittedly, it is a bit unsatisfying, that the only way of knowing that `sagawetnessindex` computes the desired terrain attributes, is to be familiar with SAGA and/or google for "SAGA catchment slope".]
`get_usage()` returns all function parameters and default values of a specific geoalgorithm.

```{r, eval=FALSE}
get_usage("saga:sagawetnessindex")
#>ALGORITHM: Saga wetness index
#>	DEM <ParameterRaster>
#>	SUCTION <ParameterNumber>
#>	AREA_TYPE <ParameterSelection>
#>	SLOPE_TYPE <ParameterSelection>
#>	SLOPE_MIN <ParameterNumber>
#>	SLOPE_OFF <ParameterNumber>
#>	SLOPE_WEIGHT <ParameterNumber>
#>	_RESAMPLING <ParameterSelection>
#>	AREA <OutputRaster>
#>	SLOPE <OutputRaster>
#>	AREA_MOD <OutputRaster>
#>	TWI <OutputRaster>
#>
#>AREA_TYPE(Type of Area)
#>	0 - [0] absolute catchment area
#>	1 - [1] square root of catchment area
#>	2 - [2] specific catchment area
#>SLOPE_TYPE(Type of Slope)
#>	0 - [0] local slope
#>	1 - [1] catchment slope
#>_RESAMPLING(Resampling method)
#>	0 - Nearest Neighbour
#>	1 - Bilinear Interpolation
#>	2 - Bicubic Spline Interpolation
#>	3 - B-Spline Interpolation
```

Subsequently, we can specify the needed parameters using R named arguments (see section \@ref(rqgis)).
Remember that we can use a `RasterLayer` living in R's global environment to specify the input raster `DEM` (see section \@ref(rqgis)).
Specifying 1 as the `SLOPE_TYPE` makes sure that the algorithm will return the catchment slope.
The resulting output rasters should be saved to temporary files with an `.sdat` extension which is a SAGA raster format.
Setting `load_output` to `TRUE` ensures that the resulting rasters will be imported into R.

```{r, eval=FALSE}
ta = run_qgis(alg = "saga:sagawetnessindex",
              DEM = dem,
              SLOPE_TYPE = 1, 
              SLOPE = tempfile(fileext = ".sdat"),
              AREA = tempfile(fileext = ".sdat"),
              load_output = TRUE,
              show_output_paths = FALSE)
```

This gives back a list with two elements named `AREA` and `SLOPE`.
Let us add to more raster elements to the list, namely `dem` and `ndvi`, and convert it into a raster stack (see section \@ref(raster-classes)), .

```{r, eval=FALSE}
ta = c(dem, ndvi, ta) %>%
  stack
names(ta) = c("dem", "ndvi", "carea", "cslope")
```

Additionally, the catchment area values are highly skewed to the right (`hist(ta$carea)`).
A log10-transformation makes the distribution more normal.

```{r, eval=FALSE}
ta$carea = log10(ta$carea)
```

```{r, eval=FALSE, echo=FALSE}
saveRDS(brick(ta), "extdata/14-ta.rds")
```

```{r, echo=FALSE}
ta = readRDS("extdata/14-ta.rds")
```

Finally, we can extract the terrain attributes to our field observations.

```{r}
data("random_points", package = "RQGIS")
random_points[, names(ta)] = raster::extract(ta, as(random_points, "Spatial"))
```

## Reducing dimensionality {#nmds}

Ordinations are a popular tool in vegetation science to extract the main information, frequently corresponding to ecological gradients, from large species-plot matrices mostly filled with 0s. 
However, they are also used in remote sensing (spectral bands + hyperspectral), the soil sciences, geomarketing, etc.
If you are unfamiliar with ordination techniques or in need of a refresher, have a look at Michael W. Palmers [webpage](http://ordination.okstate.edu/overview.htm) for a short introduction to popular ordination techniques in ecology and at @borcard_numerical_2011 for a deeper look how to apply these techniques in R. 
**vegan**'s package documentation is also very helpful (`vignette(package = "vegan")`).

Principal component analysis (PCA) is probably the most famous ordination technique. 
It is a great tool to reduce dimensionality if one can expect linear relationships between variables, and if the joint absence of a variable (for example calcium) in two plots (observations) can be considered a similarity.
This is barely the case with vegetation data.

For one, relationships are usually non-linear along environmental gradients.
That means the presence of a plant usually follows a unimodal relationship along a gradient (e.g., humidity, temperature or salinity) with a peak at the most favorable conditions and declining ends towards the unfavarable conditions. 

Secondly, the joint absence of a species in two plots is often hardly an indication for similarity.
Suppose a plant species is absent from the driest (e.g., an extreme desert) and the most moist locations (e.g., a tree savannah) of our sampling.
Then we really should refrain from counting this as a simlilarity because it is very likely that the only thing these two completely different environmental settings have in common in terms of floristic composition is the shared absence of species (except for rare ubiquist species). 

Non-metric multidimensional scaling (NMDS) is one popular dimension-reducing technique in ecology [@vonwehrden_pluralism_2009].
NMDS reduces the rank-based differences between the distances between objects in the original matrix and distances between the ordinated objects. 
The difference is expressed as stress. 
The lower the stress value, the better the ordination, i.e. the low-dimensional representation of the original matrix.
Stress values lower than 10 represent an excellent fit, stress values of around 15 are still good, and values greater than 20 represent a poor fit.
In R, `metaMDS()` of the **vegan** package can execute a NMDS.
As input it expects a community matrix with the sites as rows and the species as columns, e.g.:

```{r}
# sites 35 to 40 and corresponding species occurrences
comm[35:40, 1:5]
```

Often ordinations using presence-absence data yield better results though the prize is, of course, a less informative input matrix (see also exercises).
`decostand()` converts numerical obervations into presences and absences with 1 indicating the occurence of a species and 0 the absence of a species.

```{r}
pa = decostand(comm, "pa")
``` 

The resulting output matrix serves as input for the NMDS.
`k` specifies the number of output axes, here, set to 4.
NMDS is an iterative procedure trying to make the ordinated space more similar to the input matrix in each step.
To make sure that the algorithm converges, we set the number of steps to 500 (`try` parameter).

```{r, eval=FALSE, message=FALSE}
set.seed(25072018)
nmds = metaMDS(comm = pa, k = 4, try = 500)
nmds$stress
#>...
#>Run 498 stress 0.08834745 
#>... Procrustes: rmse 0.004100446  max resid 0.03041186 
#>Run 499 stress 0.08874805 
#>... Procrustes: rmse 0.01822361  max resid 0.08054538 
#>Run 500 stress 0.08863627 
#>... Procrustes: rmse 0.01421176  max resid 0.04985418 
#>*** Solution reached
#>0.08831395
```

```{r, eval=FALSE, echo=FALSE}
saveRDS(nmds, "extdata/14-nmds.rds")
```

```{r, include=FALSE}
nmds = readRDS("extdata/14-nmds.rds")
```

A stress value of 9 represents a very good result, which means that the reduced ordination space represents the large majority of the variance of the input matrix.
Overall, NMDS puts objects that are more similar (in terms of species composition) closer together in ordination space.
However, as opposed to most other ordination techniques, the axes are arbitrary and not necessarily ordered by importance [@borcard_numerical_2011].
However, we already know that humidity represents the main gradient in the study area [@muenchow_predictive_2013;@muenchow_rqgis:_2017].
Since humidity is highly correlated with elevation, we rotate the NMDS in accordance with elevation.
Plotting the result reveals that the first axis is, as intended, clearly associated with altitude.

```{r}
elev = dplyr::filter(random_points, id %in% rownames(pa)) %>% 
  dplyr::pull(dem)
# rotating NMDS in accordance with altitude (proxy for humidity)
rotnmds = MDSrotate(nmds, elev)
# extracting the first two axes
sc = scores(rotnmds, choices = 1:2)
par(mfrow = c(1, 2))
# plotting the first two dca axes
plot(sc, type = "n", cex.lab = 0.7)
text(sc, labels = elev, cex = 0.5)
# plotting the first axis against altitude
plot(y = sc[, 1], x = elev, xlab = "elevation in m", 
     ylab = "First NMDS axis", cex.lab = 0.7)
```

The scores of the first NMDS axis represent the different vegetations formations appearing along the slope of Mt. Mongón.
To spatially visualize them, we can model the NMDS scores with the previously created predictors, and use the resulting model for predictive mapping (see next section).

## Modeling


### Performance estimation

<!-- this should probably be only an exercise since everything has been covered in chapter spatial cv -->
The following code largely follows the steps we have introduced in section \@ref(svm).
The only differences are:

1. the response variable is numeric, hence a regression task will replace the classification task of section \@ref(svm).
1. instead of the AUROC which can only be used for categorical response variables, we will use the root mean squared error (RMSE) as performance measure.
1. we use a random forest model instead of a support vector machine which naturally goes along with different hyperparameters.

<!-- shortly introduce random forests --> 

### Predictive mapping

`r 50 * 5 * 5 * 100 + 500` models were necessary to retrieve bias-reduced performance estimates.
In the hyperparameter tuning fold, we found the best hyperparameter combination which in turn was used in the outer performance loop for predicting the test data of a specific spatial partition. 
This was done for five spatial partitions, and repeated a 100 times.
This yields in total 500 optimal hyperparameter combinations.
Which one should we use for making spatial predictions?
The answer is simple, none at all. 
Remember, the tuning was done to retrieve a bias-reduced performance estimate, not the best possible spatial prediction.
For this, one estimates the best hyperparameter combination on the complete dataset.
This mean, the inner hyperparameter tuning level is no longer needed which makes perfectly sense since we are applying our model to new data (unvisited field observations) for which the true outcomes are unavailable, hence testing is impossible. 
In short, we tune the hyperparameters for a good spatial prediction on the complete dataset via a 5-fold spatial CV with one repetition.
<!-- If we used more than one repetition (say 2) we would retrieve multiple optimal tuned hyperparameter combinations (say 2) -->

```{r, eval=FALSE}
# construct response-prediction matrix
d = data.frame(id = as.numeric(rownames(scores(rotnmds))),
               sc = scores(rotnmds)[, 1])
d = inner_join(random_points, d,  by = "id")

# extract coordinates
coords = sf::st_coordinates(d) %>% 
  as.data.frame %>%
  rename(x = X, y = Y)
# only keep variables needed for the modeling
d = dplyr::select(d, -id, -spri) %>%
  st_set_geometry(NULL)

# create task
task = makeRegrTask(data = d, target = "sc", coordinates = coords)

# learner
lrn = makeLearner(cl = "regr.ranger", predict.type = "response")
# spatial partitioning
rdesc = makeResampleDesc("SpCV", iters = 5)
# specifying the search space
ctrl = makeTuneControlRandom(maxit = 50L)
ps = makeParamSet(makeIntegerParam("mtry", lower = 1, upper = ncol(d) - 1),
                  makeIntegerParam("num.trees", lower = 10, upper = 10000))
# hyperparamter tuning
res = tuneParams(lrn, task = task,
                 resampling = rdesc,
                 par.set = ps, control = ctrl, 
                 measures = mlr::rmse)
```

This returns the best tuned hyperparameter per iteration (?) and then uses the best result from all iterations

```{r, eval=FALSE}
lrn_rf = makeLearner(cl = "regr.ranger",
                     predict.type = "response",
                     mtry = res$x$mtry, num.trees = res$x$num.trees)
# train model
model_rf = train(lrn_rf, task)
# prediction
new_d = newdata = as.data.frame(as.matrix(ta))
pred_rf = predict(model_rf, newdata = new_d)
pred_r = dem
pred_r[] = pred_rf$data$response
# restrict prediction to your study area
pred_r = mask(pred_r, study_area) %>%
  trim
blue = rgb(0, 0, 146, maxColorValue = 255)
lightblue = rgb(0, 129, 255, maxColorValue = 255)
turquoise = rgb(0, 233, 255, maxColorValue = 255)
green = rgb(142, 255, 11, maxColorValue = 255)
yellow = rgb(245, 255, 8, maxColorValue = 255)
orange = rgb(255, 173, 0, maxColorValue = 255)
lightred = rgb(255, 67, 0, maxColorValue = 255)
red = rgb(170, 0, 0, maxColorValue = 255)
pal = colorRampPalette(c(blue, lightblue, turquoise, green, yellow, 
                         orange, lightred, red))
plot(pred_r, col = pal(50))
plot(pred_r, col = RColorBrewer::brewer.pal(6, "Spectral"))

# well, this looks nice
plot(trim(mask(hs, study_area)), col = gray(0:100 / 100), legend = FALSE)
plot(pred_r, col = pal(50), alpha = 0.7, add = TRUE)
```

## Ecological context

- we have used just one axis, it would be interesting to model also the second axis and finding an innovative way of visualizing the modeled scores of the first two axis
- discussion: what could be done better or alternatives and again emphasizing that the methods shown
finally, point to books on ecological modeling especially emphasizing Alain Zuur's books


## Exercises
<!-- still, unrefined brain-storming -->
1. Run a NMDS using the percentage data of the community matrix. 
Report the stress value and compare it to the stress value as retrieved from the NMDS using presence-absence data.
What might explain the 
1. Compute catchment area and catchment slope using **RSAGA**.
1. Use profile and tangential curvature as additional predictors for the spatial prediction of the floristic gradient (hint: `grass7:r.slope.aspect`).
1. Retrieve the bias-reduced RMSE using spatial cross-validation including the estimation of optimal hyperparameter combinations in an inner tuning loop.
